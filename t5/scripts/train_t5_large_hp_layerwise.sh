python -m torch.distributed.launch --nproc_per_node=8 --master_port 9992 train_hp_layerwise.py \
--global_train_batch_size 8 \
--model_config t5-large \
--num_encoder_layer 24 \
--num_decoder_layer 24 \
--seq_length 512 \
--epochs 10 \
--lr 1e-4 \
--weight_decay 0.01 \
--dropout_prob 0.1 \
--check_loss 0 \
--pp_deg 1 \
--global_tp_deg 1 \
--global_tp_consec 1 \
--chunks 1 \
--fsdp 0 \
--profile 0 \
--apply_strategy 0